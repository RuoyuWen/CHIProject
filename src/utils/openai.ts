import OpenAI from 'openai';

export async function sendMessage(
  messages: Array<{role: string, content: string}>,
  settings: {
    apiKey: string;
    chatModel: string;
    summaryModel: string;
    language: string;
    temperature: number;
    maxTokens: number;
    downloadPath: string;
    scenePrompt: string;
    llmAPrompt: string;
    llmBPrompt: string;
    summaryPrompt: string;
    targetScene: string;
    finalTask: string;
    aiMode: string;
  } | null,
  useModel: 'chat' | 'summary' = 'chat'
) {
  // Check if settings exist
  if (!settings || !settings.apiKey) {
    throw new Error('Please configure OpenAI API key first');
  }

  // Check API key format
  if (!settings.apiKey.startsWith('sk-')) {
    throw new Error('API key format is incorrect, should start with sk-');
  }

  const openai = new OpenAI({
    apiKey: settings.apiKey,
    dangerouslyAllowBrowser: true
  });

  try {
    const selectedModel = useModel === 'chat' ? settings.chatModel : settings.summaryModel;
    const response = await openai.chat.completions.create({
      model: selectedModel,
      messages: messages as any,
      temperature: settings.temperature,
      max_tokens: settings.maxTokens,
    });

    return response.choices[0]?.message?.content || '';
  } catch (error) {
    console.error('OpenAI API Error:', error);
    
    // Provide more friendly error messages
    if (error instanceof Error) {
      if (error.message.includes('401')) {
        throw new Error('âŒ Invalid API key, please check if your OpenAI API key is correct');
      } else if (error.message.includes('429')) {
        throw new Error('â° API rate limit exceeded, please try again later');
      } else if (error.message.includes('insufficient_quota')) {
        throw new Error('ğŸ’³ Insufficient API quota, please check your OpenAI account balance');
      } else if (error.message.includes('model_not_found')) {
        throw new Error('ğŸ¤– Selected model is not available, please try another model');
      }
    }
    
    throw error;
  }
}

export async function analyzeContentIntent(
  messages: Array<{role: string, content: string}>,
  contentType: string,
  settings: {
    apiKey: string;
    chatModel: string;
    summaryModel: string;
    language: string;
    temperature: number;
    maxTokens: number;
    downloadPath: string;
    scenePrompt: string;
    llmAPrompt: string;
    llmBPrompt: string;
    summaryPrompt: string;
    targetScene: string;
    finalTask: string;
    aiMode: string;
  } | null
): Promise<{
  shouldUse: boolean;
  summary: string;
  confidence: number;
}> {
  if (!settings || !settings.apiKey) {
    return {
      shouldUse: false,
      summary: '',
      confidence: 0
    };
  }

  const openai = new OpenAI({
    apiKey: settings.apiKey,
    dangerouslyAllowBrowser: true
  });

  try {
    const analysisPrompt = `
åˆ†æä»¥ä¸‹å¯¹è¯ï¼Œåˆ¤æ–­ç”¨æˆ·æ˜¯å¦æƒ³è¦ä½¿ç”¨è®¨è®ºçš„å†…å®¹ä½œä¸º${contentType}ã€‚

å¯¹è¯å†…å®¹ï¼š
${messages.map(m => `${m.role}: ${m.content}`).join('\n')}

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{
  "shouldUse": boolean, // ç”¨æˆ·æ˜¯å¦æƒ³è¦ä½¿ç”¨è¿™ä¸ªå†…å®¹
  "summary": "string", // å¯¹è¯å†…å®¹çš„ç®€æ´æ€»ç»“ï¼ˆ50å­—ä»¥å†…ï¼‰
  "confidence": number // ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
}

åˆ¤æ–­æ ‡å‡†ï¼š
- ç”¨æˆ·æ˜ç¡®è¡¨è¾¾æƒ³è¦ä½¿ç”¨æŸä¸ªæƒ³æ³•æˆ–æ¦‚å¿µ
- ç”¨æˆ·å¯¹æŸä¸ªæè¿°è¡¨ç¤ºæ»¡æ„æˆ–è®¤å¯
- ç”¨æˆ·è¯´"å°±ç”¨è¿™ä¸ª"ã€"å¥½çš„"ã€"å¯ä»¥"ç­‰ç¡®è®¤è¯æ±‡
- ç½®ä¿¡åº¦åº”è¯¥åŸºäºç”¨æˆ·è¡¨è¾¾çš„æ˜ç¡®ç¨‹åº¦
`;

    const response = await openai.chat.completions.create({
      model: settings.summaryModel,
      messages: [
        { role: "user", content: analysisPrompt }
      ],
      temperature: 0.1,
    });

    const result = JSON.parse(response.choices[0]?.message?.content || '{}');
    return {
      shouldUse: result.shouldUse || false,
      summary: result.summary || '',
      confidence: result.confidence || 0
    };
  } catch (error) {
    console.error('Content analysis error:', error);
    return {
      shouldUse: false,
      summary: '',
      confidence: 0
    };
  }
} 